# Generador de Lexer a partir de Especificaciones YALex
Este proyecto expande la funcionalidad de construcciÃ³n de AutÃ³matas Finitos Deterministas (AFD) para crear un Lexer (analizador lÃ©xico) basado en una especificaciÃ³n YALex. El sistema combina la creaciÃ³n de DFAs â€”mediante la tÃ©cnica de followposâ€” con la lectura de definiciones YALex y la generaciÃ³n automÃ¡tica de cÃ³digo para el analizador lÃ©xico final.

## Funcionalidad Principal
1. Lectura de la especificaciÃ³n en YALex (archivo .yal).
2. ConstrucciÃ³n del Ã¡rbol de expresiÃ³n (y su graficaciÃ³n) que representa la definiciÃ³n regular de los tokens.
3. GeneraciÃ³n de un programa fuente (thelexer.py) que implementa un analizador lÃ©xico segÃºn las reglas definidas en YALex.
4. Posibilidad de probar el lexer resultante con un script de ejemplo, verificando los tokens reconocidos.

## ğŸ“ Estructura del Proyecto
ğŸ“„ ```main.py``` â†’ Funciona como punto de entrada o script principal

ğŸ“„ ```thelexer.py``` â†’ (Generado automÃ¡ticamente) Contiene la clase Lexer final. Se construye combinando cada regla y su DFA correspondiente para reconocer tokens. No se edita manualmente.

ğŸ“„ ```run_lexer.py``` â†’ Script que genera (o actualiza) el analizador lÃ©xico a partir de la especificaciÃ³n YALex y luego ejecuta dicho lexer sobre un archivo de texto dado (por defecto, entrada.txt).

### ğŸ“‚ models/
- ğŸ“„ ```regex_parser.py``` â†’ Convierte expresiones regulares en notaciÃ³n postfija (usando una variante del algoritmo Shunting-Yard).
- ğŸ“„ ```syntax_tree.py``` â†’ Construye y representa el Ã¡rbol sintÃ¡ctico a partir de la notaciÃ³n postfija. TambiÃ©n permite graficarlo con Graphviz.
- ğŸ“„ ```dfa.py``` â†’ Implementa la construcciÃ³n directa de un AFD usando la tÃ©cnica de followpos, e incluye mÃ©todos de simulaciÃ³n.
- ğŸ“„ ```mindfa.py``` â†’ Aplica el algoritmo de Hopcroft para minimizar el AFD resultante.

### ğŸ“‚ inputs/
- ğŸ“„ ```yalex_parser.py``` â†’ Parsea un archivo .yal (YALex) para extraer definiciones y reglas.
- ğŸ“„ ```entrada.txt``` â†’ Archivo de entrada que contiene las cadenas (texto) a ser procesadas y tokenizadas por el analizador lÃ©xico.  

### ğŸ“‚ controllers/
- ğŸ“„ ```main_controller.py``` â†’ Orquestador principal. Genera el DFA global a partir de YALex, asigna marcadores a cada regla y construye la clase Lexer.

### ğŸ“‚ tests/
- ğŸ“„ ```test_lexer.py``` â†’ Ejemplo de script para probar el lexer generado. Lee una cadena de ejemplo y muestra los tokens generados.

## ğŸ›  TecnologÃ­as Utilizadas
- Python â†’ Lenguaje principal del proyecto.
- Graphviz â†’ VisualizaciÃ³n de Ã¡rboles sintÃ¡cticos y AFDs.
- Shunting-Yard â†’ ConversiÃ³n de expresiones regulares a notaciÃ³n postfija.
- Hopcroft â†’ MinimizaciÃ³n de AFD.
- YALex â†’ EspecificaciÃ³n de reglas lÃ©xicas en un archivo .yal.

## âš™ï¸ InstalaciÃ³n y Uso
1. **Clona el repositorio**:
    ```
   git clone <repository-url>
    ```
2. **Navega al directorio**:
   ```
   cd <repository-name>
   ```
3. **Instala las dependencias**:
    ```
   pip install graphviz
    ```
4. **Preparar el archivo .yal**:

   En la carpeta inputs/, ubica o edita el archivo lexer.yal con tus definiciones y reglas.
5. **Generar el lexer**:
   Desde tu terminal:
    ```
   python run_lexer.py
    ```
    El programa leerÃ¡ el texto de entrada.txt, reconocerÃ¡ los tokens definidos en el .yal y mostrarÃ¡ en pantalla tanto los tokens identificados como los errores lÃ©xicos, de existir.

### Ejemplo de Archivo YALex
  ```
{ 
    # CÃ³digo de header (opcional)
}
    let DIGIT = [0-9]
    let LETTER = [A-Za-z]
    rule gettoken =
        | LETTER (LETTER | DIGIT)+ { return ("WORD", lexeme) }
        | LETTER { return ("LETTER", lexeme) }
        | [' ' '\t'] { return None }
        | eof    { return ("EOF", None) }
        | '\n' { return ("EOL", None) }
{ 
    # CÃ³digo de trailer (opcional)
}
  ```

### Cuando se ejecute, el sistema:
1. Leer el archivo YALex:
   Se extraen definiciones, reglas y el header o trailer opcional.
2. Construir un DFA global que reconoce todos los tokens, asignando un marcador Ãºnico a cada regla.
3. Generar (o reutilizar) DFAs especÃ­ficos para cada regla y construir el archivo thelexer.py.
4. Usar el lexer en scripts de prueba, identificando tokens en la cadena de entrada.

## Ejemplo de Entrada y Salida
Entrada:
```
Hola como 45 X

A + B
```

Salida Esperada:

```[('WORD', 'Hola'), ('WORD', 'Como'), ('NUMBER', 45), ('LETTER', 'X'), ('EOL', None), ('EOL', None), ('LETTER', 'A'), ('PLUS', '+'), ('LETTER', 'B'), ('EOL', None)]```

## ğŸ“š Referencias
#### Graphviz - Graph Visualization Software
ğŸ”—[Graphviz Documentation](https://graphviz.org/)
#### Regular Expressions - MDN Web Docs
ğŸ”—[Mozilla Developer Network (MDN)]( https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_expressions)
#### IBM i 7.3 - Regular Expressions
ğŸ”—[IBM Documentation](https://www.ibm.com/docs/es/i/7.3?topic=expressions-regular)

## âš–ï¸ Licencia
ğŸ“Œ UVG License
Proyecto de cÃ³digo abierto bajo la licencia UVG. Puedes usarlo, modificarlo y distribuirlo libremente dando crÃ©dito a los autores originales.
